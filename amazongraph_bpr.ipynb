{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.utils import to_undirected, negative_sampling, add_self_loops\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import to_undirected, negative_sampling, structured_negative_sampling\n",
    "from torch_geometric.data import Data\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "from torch_geometric.nn import GraphSAGE, GAT, GIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_co_purchase_df = pd.read_csv('kcore70new.csv')\n",
    "processed_co_purchase_df = pd.read_csv('kcore5new.csv')\n",
    "processed_co_purchase_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_list = list(processed_co_purchase_df['asin'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = np.load(\"image_embeddings_k_5.npy\")\n",
    "text_embeddings = np.load(\"text_embeddings_k_5.npy\")\n",
    "edge_index = np.load(\"k5_edge_index.npy\")\n",
    "\n",
    "# image_embeddings = np.load(\"image_embeddings.npy\")\n",
    "# text_embeddings = np.load(\"text_embeddings.npy\")\n",
    "# edge_index = np.load(\"kcore70newedges.npy\")\n",
    "random_embeddings = torch.randn_like(torch.tensor(text_embeddings.astype(np.float32))) / 10\n",
    "\n",
    "combined_embedings = np.concatenate((image_embeddings, text_embeddings), axis = 1)\n",
    "product_feature_dim = 1024\n",
    "num_products = len(asin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "\n",
    "data = Data(x=torch.tensor(combined_embedings.astype(np.float32)), edge_index=torch.tensor(edge_index))\n",
    "# data = Data(x=combined_embedings, edge_index=torch.tensor(edge_index))\n",
    "\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_edge_index_to_adj_list(edge_index):\n",
    "    adj_list = defaultdict(list)\n",
    "    for src, dest in zip(edge_index[0], edge_index[1]):\n",
    "        adj_list[src.item()].append(dest.item())\n",
    "    return adj_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first hop, we sample at most 20 neighbors.\n",
    "# In the second hop, we sample at most 10 neighbors.\n",
    "# In addition, during training, we want to sample negative edges on-the-fly with\n",
    "# a ratio of 2:1.\n",
    "# We can make use of the `loader.LinkNeighborLoader` from PyG:\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# For this, we first split the set of edges into\n",
    "# training (80%), validation (10%), and testing edges (10%).\n",
    "# Across the training edges, we use 70% of edges for message passing,\n",
    "# and 30% of edges for supervision.\n",
    "# We further want to generate fixed negative edges for evaluation with a ratio of 2:1.\n",
    "# Negative edges during training will be generated on-the-fly.\n",
    "# We can leverage the `RandomLinkSplit()` transform for this from PyG:\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.05,\n",
    "    num_test=0.05,\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples = False,\n",
    "    neg_sampling_ratio=0.0,\n",
    "    disjoint_train_ratio=0.2)\n",
    "    \n",
    "train_data, val_data, test_data = transform(data)\n",
    "\n",
    "# Define seed edges:\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[10, 10, 10],\n",
    "    edge_label_index=train_data.edge_label_index,\n",
    "    edge_label=train_data.edge_label,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = LinkNeighborLoader(\n",
    "    data = val_data, \n",
    "    num_neighbors=[10, 10, 10],\n",
    "    edge_label_index=val_data.edge_label_index,\n",
    "    edge_label=val_data.edge_label,\n",
    "    batch_size = 1,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_loader = LinkNeighborLoader(\n",
    "    data = test_data, \n",
    "    num_neighbors=[10, 10, 10],\n",
    "    edge_label_index=test_data.edge_label_index,\n",
    "    edge_label=test_data.edge_label,\n",
    "    batch_size = 1,\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_edge_index_to_adj_list(edge_index):\n",
    "    adj_list = defaultdict(list)\n",
    "    for src, dest in zip(edge_index[0], edge_index[1]):\n",
    "        adj_list[src.item()].append(dest.item())\n",
    "    return adj_list\n",
    "\n",
    "def bfs_sample(adj_list, start_node, max_depth):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node, 0)])\n",
    "    visited.add(start_node)\n",
    "    \n",
    "    while queue:\n",
    "        current_node, depth = queue.popleft()\n",
    "        if depth == max_depth:\n",
    "            return current_node\n",
    "        \n",
    "        for neighbor in adj_list[current_node]:\n",
    "            if neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append((neighbor, depth + 1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "def add_bfs_negatives(data, m=3, k=3):\n",
    "    adj_list = convert_edge_index_to_adj_list(data.edge_index)\n",
    "    new_edges = []\n",
    "    new_labels = []\n",
    "    new_edge_label_index = []\n",
    "\n",
    "    for edge in tqdm(range(data.edge_label_index.size(1))):\n",
    "        start_nodes = [data.edge_label_index[0, edge].item(), data.edge_label_index[1, edge].item()]\n",
    "        for start_node in start_nodes:\n",
    "            for _ in range(k):\n",
    "                depth = random.randint(2, m)\n",
    "                neg_node = bfs_sample(adj_list, start_node, depth)\n",
    "                if neg_node is not None and neg_node not in adj_list[start_node]:\n",
    "                    new_edges.append([start_node, neg_node])\n",
    "                    new_labels.append(0)  # Label for negative samples\n",
    "                    new_edge_label_index.append([start_node, neg_node])\n",
    "\n",
    "    new_edges = torch.tensor(new_edges, dtype=torch.long).t()\n",
    "    new_labels = torch.tensor(new_labels, dtype=torch.float)\n",
    "    new_edge_label_index = torch.tensor(new_edge_label_index, dtype=torch.long).t()\n",
    "\n",
    "    data.edge_index = torch.cat([data.edge_index, new_edges], dim=1)\n",
    "    data.edge_label = torch.cat([data.edge_label, new_labels])\n",
    "    data.edge_label_index = torch.cat([data.edge_label_index, new_edge_label_index], dim=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_negative_samples(data, negative_samples):\n",
    "    \"\"\"\n",
    "    Appends negative edge samples to the data object.\n",
    "\n",
    "    Args:\n",
    "    data (Data): The data object containing graph information.\n",
    "    negative_samples (tuple): A tuple of (i, k) where (i, k) are negative edges.\n",
    "\n",
    "    Returns:\n",
    "    Updated data object with negative samples appended.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the negative samples (only i and k are present)\n",
    "    i, k = negative_samples\n",
    "\n",
    "    # Create a tensor of zeros for the negative edge labels\n",
    "    negative_edge_labels = torch.zeros(i.size(0), dtype=data.edge_label.dtype)\n",
    "\n",
    "    # Append the negative edge labels to the edge_label attribute\n",
    "    data.edge_label = torch.cat([data.edge_label, negative_edge_labels.to(data.edge_label.device)], dim=0)\n",
    "\n",
    "    # Create the negative edge label index (i, k)\n",
    "    negative_edge_label_index = torch.stack([i, k], dim=0)\n",
    "\n",
    "    # Append the negative edge label index to the edge_label_index attribute\n",
    "    data.edge_label_index = torch.cat([data.edge_label_index, negative_edge_label_index], dim=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(data, num_neg_samples):\n",
    "    \"\"\"\n",
    "    Generates and appends negative edge samples to the data object.\n",
    "\n",
    "    Args:\n",
    "    data (Data): The data object containing graph information.\n",
    "    num_neg_samples (int): The number of negative samples to generate.\n",
    "\n",
    "    Returns:\n",
    "    Updated data object with negative samples appended.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure num_neg_samples is valid\n",
    "    if num_neg_samples < 1:\n",
    "        raise ValueError(\"Number of negative samples must be at least 1.\")\n",
    "\n",
    "    all_i = []\n",
    "    all_k = []\n",
    "\n",
    "    for _ in range(num_neg_samples):\n",
    "        # Generate negative samples\n",
    "        i, _, k = structured_negative_sampling(data.edge_label_index, data.num_nodes)\n",
    "        \n",
    "        # Aggregate the i and k components of the negative samples\n",
    "        all_i.append(i)\n",
    "        all_k.append(k)\n",
    "\n",
    "    # Combine all i and k components\n",
    "    combined_i = torch.cat(all_i, dim=0)\n",
    "    combined_k = torch.cat(all_k, dim=0)\n",
    "    combined_negative_samples = (combined_i, combined_k)\n",
    "\n",
    "    # Append the combined negative samples to the data object\n",
    "    return append_negative_samples(data, combined_negative_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_generate_negative_samples(data, num_neg_samples):\n",
    "    \"\"\"\n",
    "    Generates and appends negative edge samples to the data object.\n",
    "\n",
    "    Args:\n",
    "    data (Data): The data object containing graph information.\n",
    "    num_neg_samples (int): The number of negative samples to generate.\n",
    "\n",
    "    Returns:\n",
    "    Updated data object with negative samples appended.\n",
    "    \"\"\"\n",
    "\n",
    "    new_edge_label_index = []\n",
    "    new_edge_labels = []\n",
    "\n",
    "    for pos_edge in range(data.edge_label_index.size(1)):\n",
    "        # Keep the positive edge\n",
    "        new_edge_label_index.append(data.edge_label_index[:, pos_edge].view(2, 1))  # Reshape for consistency\n",
    "        new_edge_labels.append(data.edge_label[pos_edge].view(1))\n",
    "\n",
    "        # Generate negative samples for this positive edge\n",
    "        for _ in range(num_neg_samples):\n",
    "            _, _, neg_edge = structured_negative_sampling(data.edge_label_index[:, [pos_edge]], data.num_nodes)\n",
    "            new_edge_label_index.append(neg_edge.view(2, 1))  # Reshape for consistency\n",
    "            new_edge_labels.append(torch.tensor([0], dtype=data.edge_label.dtype))  # Negative label\n",
    "\n",
    "    # Concatenate all new edges and labels\n",
    "    data.edge_label_index = torch.cat(new_edge_label_index, dim=1)\n",
    "    data.edge_label = torch.cat(new_edge_labels, dim=0)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GraphSAGE(\n",
    "    in_channels=product_feature_dim,\n",
    "    hidden_channels=256,\n",
    "    out_channels=256,\n",
    "    dropout = 0.1,\n",
    "    project = True,\n",
    "    num_layers=3).to(device)\n",
    "    \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GAT(\n",
    "    in_channels=product_feature_dim,\n",
    "    hidden_channels=256,\n",
    "    out_channels=256,\n",
    "    num_layers = 3,\n",
    "    dropout = 0.02,\n",
    "    heads=4).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GIN(\n",
    "    in_channels = product_feature_dim, \n",
    "    hidden_channels=256,\n",
    "    out_channels = 256,\n",
    "    dropout = 0.1,\n",
    "    num_layers = 3\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"iter_SAGE_final_2\"  # replace with your experiment name\n",
    "num_iters = 500\n",
    "logging_freq = 50\n",
    "num_val_batch_sample = 30\n",
    "num_epochs = 1\n",
    "num_neg_sample = 4\n",
    "top_k = 3\n",
    "\n",
    "\n",
    "os.makedirs(experiment_name, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=experiment_name)\n",
    "\n",
    "\n",
    "def recover_logits(model, batch):\n",
    "    z = model(batch.x, batch.edge_index)\n",
    "\n",
    "    head_embeddings = z[batch.edge_label_index[0]]\n",
    "    head_embeddings = head_embeddings / head_embeddings.norm(dim = 1, keepdim = True) #think about adding L2 Norm after every layer if applicable. is important. \n",
    "\n",
    "    tail_embeddings = z[batch.edge_label_index[1]]\n",
    "    tail_embeddings = tail_embeddings / tail_embeddings.norm(dim = 1, keepdim = True)\n",
    "\n",
    "\n",
    "    link_logits = (head_embeddings * tail_embeddings).sum(dim=1)\n",
    "\n",
    "\n",
    "    return link_logits\n",
    "\n",
    "\n",
    "def get_embeddings(model, batch):\n",
    "    z = model(batch.x, batch.edge_index)\n",
    "    z = z / z.norm(dim=1, keepdim=True)  # Normalizing embeddings\n",
    "    return z\n",
    "\n",
    "def compute_dot_products(embeddings, head_node_index):\n",
    "    head_embedding = embeddings[head_node_index]\n",
    "    dot_products = torch.matmul(embeddings, head_embedding.unsqueeze(-1)).squeeze(-1)\n",
    "    return dot_products\n",
    "\n",
    "\n",
    "def recall_at_k(dot_products, actual_tail_index, k=10):\n",
    "    print(dot_products.shape)\n",
    "    top_k_scores, top_k_indices = torch.topk(dot_products, k)\n",
    "    return 1 if actual_tail_index in top_k_indices else 0\n",
    "\n",
    "\n",
    "\n",
    "def bpr_loss(pos_score, neg_scores):\n",
    "    return -torch.log(torch.sigmoid(pos_score - neg_scores)).mean()\n",
    "\n",
    "total_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "        if i % logging_freq == 0:\n",
    "            recall_at_k_scores = []\n",
    "\n",
    "            for _ in range(num_val_batch_sample):\n",
    "                val_sample = next(iter(val_loader))\n",
    "                val_sample.to(device)\n",
    "\n",
    "                embeddings = get_embeddings(model, val_sample)\n",
    "\n",
    "                head_node_index = val_sample.edge_label_index[0, 0]  # Assuming single label per batch\n",
    "                actual_tail_index = val_sample.edge_label_index[1, 0]\n",
    "\n",
    "                dot_products = compute_dot_products(embeddings, head_node_index)\n",
    "                recall_at_k_score = recall_at_k(dot_products, actual_tail_index, k=top_k)\n",
    "\n",
    "                recall_at_k_scores.append(recall_at_k_score)\n",
    "\n",
    "\n",
    "            # Average the scores\n",
    "\n",
    "            recallk = np.mean(recall_at_k_scores)\n",
    "            \n",
    "            writer.add_scalar('Recall at K', recallk, i)\n",
    "\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(experiment_name, f'checkpoint_{i}.pth'))\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "            batch = generate_negative_samples(batch, num_neg_sample)  # Generate 2 negative samples per positive sample\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = recover_logits(model, batch)\n",
    "\n",
    "            # BPR loss computation\n",
    "            loss = 0.0\n",
    "            num_pos_samples = len(batch.edge_label) // (num_neg_sample + 1)  # Total number of positive samples\n",
    "            for j in range(num_pos_samples):\n",
    "                pos_logit = logits[j]\n",
    "                neg_logits = []\n",
    "                for n in range(num_neg_sample):\n",
    "                    neg_index = num_pos_samples + j + n * num_pos_samples\n",
    "                    neg_logits.append(logits[neg_index])\n",
    "                neg_logits = torch.stack(neg_logits)\n",
    "                loss += bpr_loss(pos_logit, neg_logits)\n",
    "\n",
    "            loss = loss / num_pos_samples\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            writer.add_scalar('Loss', loss.item(), i)\n",
    "\n",
    "        if i == num_iters:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for calculating average recall at k\n",
    "total_recall_at_k = 0\n",
    "num_test_batches = 0\n",
    "\n",
    "# Iterate through the test_loader\n",
    "for test_batch in test_loader:\n",
    "    num_test_batches += 1\n",
    "    test_batch.to(device)\n",
    "\n",
    "    # Get embeddings for the test batch\n",
    "    embeddings = get_embeddings(model, test_batch)\n",
    "\n",
    "    # Assuming you are interested in the first edge in each batch for recall calculation\n",
    "    head_node_index = test_batch.edge_label_index[0, 0] \n",
    "    actual_tail_index = test_batch.edge_label_index[1, 0]\n",
    "\n",
    "    # Compute dot products and recall at k for the test batch\n",
    "    dot_products = compute_dot_products(embeddings, head_node_index)\n",
    "    recall_at_k_score = recall_at_k(dot_products, actual_tail_index, k=top_k)  # You can adjust k as needed\n",
    "\n",
    "    # Update total recall at k score\n",
    "    total_recall_at_k += recall_at_k_score\n",
    "\n",
    "    if num_test_batches == 200:\n",
    "        break\n",
    "\n",
    "# Calculate and print the average recall at k\n",
    "average_recall_at_k = total_recall_at_k / num_test_batches\n",
    "print(f'Average Recall at K: {average_recall_at_k}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
